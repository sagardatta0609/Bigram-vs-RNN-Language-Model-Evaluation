{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeBCoiLj6ctJ",
        "outputId": "c4f2d75e-17c6-4c42-e8aa-4c9ee14b9fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram perplexity on test: 24.929\n",
            "Epoch 1 train_loss=10.4700 test_ppl=35.817\n",
            "Epoch 2 train_loss=7.8277 test_ppl=49.695\n",
            "Epoch 3 train_loss=4.6931 test_ppl=105.430\n",
            "Bigram generations: ['the next word', 'the quick brown fox jumps over the next word', 'the lazy dog']\n",
            "RNN generations: ['the quick brown language models fox jumps the the jumps over word', 'n-gram rnns are', 'and models the lazy word']\n",
            "Bigram BLEU: 0\n",
            "RNN BLEU: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "# Short BLEU + Perplexity example for Bigram and RNN (paste into one Colab cell)\n",
        "# Uncomment if needed:\n",
        "# !pip install --quiet torch nltk\n",
        "\n",
        "import math, random\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# --- tiny dataset ---\n",
        "corpus = [\n",
        "  \"The quick brown fox jumps over the lazy dog\",\n",
        "  \"I love natural language processing\",\n",
        "  \"Language models predict the next word\",\n",
        "  \"RNNs and n-gram models are classic\",\n",
        "  \"This is a small test corpus\"\n",
        "]\n",
        "random.shuffle(corpus)\n",
        "train, val, test = corpus[:3], corpus[3:4], corpus[4:]\n",
        "\n",
        "tok = lambda s: word_tokenize(s.lower())\n",
        "train_t = [tok(s) for s in train]\n",
        "val_t = [tok(s) for s in val]\n",
        "test_t = [tok(s) for s in test]\n",
        "\n",
        "# --- vocab ---\n",
        "special = ['<pad>','<unk>','<bos>','<eos>']\n",
        "counter = Counter(w for s in (train_t+val_t+test_t) for w in s)\n",
        "itos = special + [w for w,c in counter.items()]\n",
        "stoi = {w:i for i,w in enumerate(itos)}\n",
        "\n",
        "def encode(sent):\n",
        "    return [stoi.get(w, stoi['<unk>']) for w in (['<bos>'] + sent + ['<eos>'])]\n",
        "\n",
        "# ---------------- Bigram LM (add-k smoothing) ----------------\n",
        "class Bigram:\n",
        "    def __init__(self, add_k=0.1):\n",
        "        self.counts = defaultdict(Counter)\n",
        "        self.ctx = Counter()\n",
        "        self.add_k = add_k\n",
        "        self.vocab = set()\n",
        "    def train(self, sents):\n",
        "        for s in sents:\n",
        "            s2 = ['<bos>'] + s + ['<eos>']\n",
        "            for a,b in zip(s2, s2[1:]):\n",
        "                self.counts[a][b] += 1\n",
        "                self.ctx[a] += 1\n",
        "                self.vocab.add(a); self.vocab.add(b)\n",
        "    def prob(self, a,b):\n",
        "        V = len(self.vocab)\n",
        "        return (self.counts[a][b] + self.add_k) / (self.ctx[a] + self.add_k * V)\n",
        "    def sent_logprob(self, s):\n",
        "        s2 = ['<bos>'] + s + ['<eos>']\n",
        "        lp=0.0\n",
        "        for a,b in zip(s2, s2[1:]):\n",
        "            lp += math.log(self.prob(a,b))\n",
        "        return lp\n",
        "    def perplexity(self, sents):\n",
        "        N=0; lp=0.0\n",
        "        for s in sents:\n",
        "            N += len(s)+1\n",
        "            lp += self.sent_logprob(s)\n",
        "        return math.exp(-lp/N)\n",
        "    def generate(self, maxlen=15):\n",
        "        cur='<bos>'; out=[]\n",
        "        for _ in range(maxlen):\n",
        "            choices=list(self.counts[cur].keys()) or list(self.vocab)\n",
        "            probs=[self.prob(cur,c) for c in choices]\n",
        "            probs=np.array(probs); probs/=probs.sum()\n",
        "            nxt=np.random.choice(choices,p=probs)\n",
        "            if nxt=='<eos>': break\n",
        "            out.append(nxt); cur=nxt\n",
        "        return out\n",
        "\n",
        "bg = Bigram(add_k=0.1)\n",
        "bg.train(train_t)\n",
        "print(\"Bigram perplexity on test:\", round(bg.perplexity(test_t),3))\n",
        "\n",
        "# ---------------- Small RNN LM ----------------\n",
        "class LMDataset(Dataset):\n",
        "    def __init__(self,sents):\n",
        "        self.data=[encode(s) for s in sents]\n",
        "    def __len__(self): return sum(len(x)-1 for x in self.data)\n",
        "    def __getitem__(self,idx):\n",
        "        # flatten into sequence of (input_ids, target)\n",
        "        cum=0\n",
        "        for seq in self.data:\n",
        "            n = len(seq)-1\n",
        "            if idx < cum + n:\n",
        "                i = idx - cum\n",
        "                return torch.tensor(seq[:i+1],dtype=torch.long), torch.tensor(seq[i+1],dtype=torch.long)\n",
        "            cum += n\n",
        "        raise IndexError\n",
        "def collate(batch):\n",
        "    xs,ys=zip(*batch)\n",
        "    L=max(len(x) for x in xs)\n",
        "    X = torch.full((len(xs),L), stoi['<pad>'], dtype=torch.long)\n",
        "    for i,x in enumerate(xs): X[i,-len(x):]=x  # right-align\n",
        "    return X, torch.stack(ys)\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self,vocab_size,emb=64,hidden=128):\n",
        "        super().__init__()\n",
        "        self.e=nn.Embedding(vocab_size,emb,padding_idx=0)\n",
        "        self.rnn=nn.LSTM(emb,hidden,batch_first=True)\n",
        "        self.fc=nn.Linear(hidden,vocab_size)\n",
        "    def forward(self,x):\n",
        "        emb=self.e(x)\n",
        "        out,_=self.rnn(emb)\n",
        "        return self.fc(out[:,-1,:])  # predict next token\n",
        "\n",
        "vocab_size = len(itos)\n",
        "train_ds = LMDataset(train_t)\n",
        "val_ds = LMDataset(val_t)\n",
        "test_ds = LMDataset(test_t)\n",
        "train_loader = DataLoader(train_ds,batch_size=8,shuffle=True,collate_fn=collate)\n",
        "test_loader = DataLoader(test_ds,batch_size=8,collate_fn=collate)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = RNNLM(vocab_size).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "# tiny training\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total=0.0\n",
        "    for X,y in train_loader:\n",
        "        X=X.to(device); y=y.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = crit(logits, y)\n",
        "        loss.backward(); opt.step()\n",
        "        total += loss.item()\n",
        "    # quick val perplexity:\n",
        "    model.eval()\n",
        "    nll=0.0; toks=0\n",
        "    with torch.no_grad():\n",
        "        for X,y in test_loader:\n",
        "            X=X.to(device); y=y.to(device)\n",
        "            logits = model(X)\n",
        "            loss = nn.CrossEntropyLoss(reduction='sum')(logits,y)\n",
        "            nll += loss.item(); toks += X.size(0)\n",
        "    ppl = math.exp(nll / toks) if toks>0 else float('inf')\n",
        "    print(f\"Epoch {epoch+1} train_loss={total:.4f} test_ppl={ppl:.3f}\")\n",
        "\n",
        "# RNN generation helper\n",
        "def gen_rnn(model, maxlen=12):\n",
        "    model.eval()\n",
        "    ids = [stoi['<bos>']]\n",
        "    hidden=None\n",
        "    out=[]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(maxlen):\n",
        "            x = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "            emb = model.e(x)\n",
        "            out_r, hidden = model.rnn(emb, hidden)\n",
        "            logits = model.fc(out_r[:, -1, :])\n",
        "            probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
        "            nxt = np.random.choice(len(probs), p=probs/probs.sum())\n",
        "            token = itos[nxt]\n",
        "            if token=='<eos>': break\n",
        "            out.append(token); ids.append(nxt)\n",
        "    return out\n",
        "\n",
        "# generate examples\n",
        "gen_bg = [' '.join(bg.generate()) for _ in range(3)]\n",
        "gen_rnn = [' '.join(gen_rnn(model)) for _ in range(3)]\n",
        "print(\"Bigram generations:\", gen_bg)\n",
        "print(\"RNN generations:\", gen_rnn)\n",
        "\n",
        "# --- BLEU: compare generated sentences to test references ---\n",
        "refs = [[s] for s in test_t]                # list of [reference_tokens]\n",
        "hyps_bg = [g.split() for g in gen_bg][:len(refs)]\n",
        "hyps_rnn = [g.split() for g in gen_rnn][:len(refs)]\n",
        "print(\"Bigram BLEU:\", round(corpus_bleu(refs, hyps_bg),4))\n",
        "print(\"RNN BLEU:\", round(corpus_bleu(refs, hyps_rnn),4))"
      ]
    }
  ]
}